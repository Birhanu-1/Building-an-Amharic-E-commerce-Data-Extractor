{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52c87a0-95d2-4a30-ac0c-bc460bb74930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ef6173-78d9-43ee-9244-f1adbf0672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already extracted.\n",
      "Loaded messages: (3844, 6)\n",
      "      channel title channel username     id  \\\n",
      "0  AwasMart-áŠ á‹‹áˆµáˆ›áˆ­á‰µğŸ        @AwasMart  11864   \n",
      "1  AwasMart-áŠ á‹‹áˆµáˆ›áˆ­á‰µğŸ        @AwasMart  11863   \n",
      "2  AwasMart-áŠ á‹‹áˆµáˆ›áˆ­á‰µğŸ        @AwasMart  11862   \n",
      "3  AwasMart-áŠ á‹‹áˆµáˆ›áˆ­á‰µğŸ        @AwasMart  11861   \n",
      "4  AwasMart-áŠ á‹‹áˆµáˆ›áˆ­á‰µğŸ        @AwasMart  11860   \n",
      "\n",
      "                                             message                 date  \\\n",
      "0  ğŸ¤ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½ğŸš½\\nâ‡ï¸Baby toilet seat/ potty seat\\n...  2025-06-22 11:55:19   \n",
      "1  ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–ğŸ”–\\ná‹á‹µ á‹°áŠ•á‰ áŠá‰»á‰½áŠ• áˆ±á‰ƒá‰½áŠ• áŠáŒˆ áŠ¥áˆá‹µ áŠ¨4:30-11:...  2025-06-21 16:43:54   \n",
      "2                  ğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸª\\nâ‡ï¸Baby suction bowl  2025-06-21 16:43:12   \n",
      "3                                                NaN  2025-06-21 16:43:12   \n",
      "4  ğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸª\\nâ‡ï¸Baby suction bowl\\n\\nğŸ‘‰ á‰£áˆˆ áˆáˆˆá‰µ ...  2025-06-21 16:42:32   \n",
      "\n",
      "                            media path  \n",
      "0  ../data1/photos\\@AwasMart_11864.jpg  \n",
      "1  ../data1/photos\\@AwasMart_11863.jpg  \n",
      "2                                  NaN  \n",
      "3  ../data1/photos\\@AwasMart_11861.jpg  \n",
      "4  ../data1/photos\\@AwasMart_11860.jpg  \n",
      "Saved to /data1/preprocessed_telegram_data.csv\n",
      "Messages with price mentions: 1000\n",
      "Messages with location mentions: 1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11068\\2457185132.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  price_mentions = df[\"clean_text\"].str.contains(r\"(á‰¥áˆ­|\\d+\\s*birr|\\d+\\s*á‰¥áˆ­)\", na=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11068\\2457185132.py:46: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  location_mentions = df[\"clean_text\"].str.contains(r\"(áŠ á‹²áˆµ\\s?áŠ á‰ á‰£|á‰¦áˆŒ|áˆ˜áŒˆáŠ“áŠ›)\", na=False)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define paths ---\n",
    "BASE_DIR = Path(\"../data1\")\n",
    "ZIP_PATH = BASE_DIR / \"photos.zip\"\n",
    "IMAGES_DIR = BASE_DIR / \"photos\"\n",
    "DATA_FILE = BASE_DIR / \"telegram_data.csv\"\n",
    "\n",
    "# --- Step 2: Unzip images ---\n",
    "if not IMAGES_DIR.exists():\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(IMAGES_DIR)\n",
    "    print(\"Images extracted.\")\n",
    "else:\n",
    "    print(\"Images already extracted.\")\n",
    "\n",
    "# --- Step 3: Load Telegram data ---\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "except:\n",
    "    df = pd.read_json(DATA_FILE, lines=True)\n",
    "\n",
    "df = df.rename(columns=str.lower)\n",
    "print(\"Loaded messages:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# --- Step 4: Normalize Amharic text ---\n",
    "def clean_amharic(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#df[\"clean_text\"] = df[\"text\"].apply(clean_amharic)\n",
    "df[\"clean_text\"] = df[\"message\"].apply(clean_amharic)\n",
    "\n",
    "df[\"has_image\"] = df[\"media path\"].apply(lambda x: os.path.exists(IMAGES_DIR / str(x)) if pd.notna(x) else False)\n",
    "\n",
    "# --- Step 5: Export cleaned data ---\n",
    "df_filtered = df[[\"id\", \"clean_text\", \"media path\", \"has_image\"]]\n",
    "df_filtered.to_csv(\"../data1/preprocessed_telegram_data.csv\", index=False)\n",
    "print(\"Saved to /data1/preprocessed_telegram_data.csv\")\n",
    "\n",
    "# --- Step 6: Optional stats ---\n",
    "price_mentions = df[\"clean_text\"].str.contains(r\"(á‰¥áˆ­|\\d+\\s*birr|\\d+\\s*á‰¥áˆ­)\", na=False)\n",
    "location_mentions = df[\"clean_text\"].str.contains(r\"(áŠ á‹²áˆµ\\s?áŠ á‰ á‰£|á‰¦áˆŒ|áˆ˜áŒˆáŠ“áŠ›)\", na=False)\n",
    "\n",
    "print(\"Messages with price mentions:\", price_mentions.sum())\n",
    "print(\"Messages with location mentions:\", location_mentions.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfe02cf-bc0d-4f9b-b56e-b48680529c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['channel title', 'channel username', 'id', 'message', 'date',\n",
      "       'media path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0856b5d-f6ab-4b90-8e39-a047768e22d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
